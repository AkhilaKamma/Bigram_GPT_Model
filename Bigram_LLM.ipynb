{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cCBDtzrvjpw3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn #for inializing the neural network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "Unzl_rr2Ohoe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
        "block_size = 8 #length of the integer\n",
        "batch_size = 4 #how many blocks running in parallel\n",
        "device\n",
        "max_iters = 1000\n",
        "# eval_interval = 2500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 250"
      ],
      "metadata": {
        "id": "9vZltv6TlD-h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vocab size:** How many unique characters in our dataset."
      ],
      "metadata": {
        "id": "hNQC4Q5gtxro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = sorted(set(text))\n",
        "print(chars)\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4fakCellMag",
        "outputId": "0849cae3-9296-4193-fb3c-0e836ac973e4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "PDf_bKzUlusq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print('inputs:')\n",
        "# print(x.shape)\n",
        "print(x)\n",
        "print('targets:')\n",
        "print(y)"
      ],
      "metadata": {
        "id": "lKIsiWB6lxuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd891935-42ac-4f12-c232-16317fa27061"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "tensor([[74, 65, 57,  1, 64, 67, 68, 76],\n",
            "        [67, 57,  1, 61, 58, 65, 57,  1],\n",
            "        [76,  1, 50, 58, 55,  1, 72, 62],\n",
            "        [71, 55, 65, 58,  1, 76, 54, 62]])\n",
            "targets:\n",
            "tensor([[65, 57,  1, 64, 67, 68, 76,  1],\n",
            "        [57,  1, 61, 58, 65, 57,  1, 72],\n",
            "        [ 1, 50, 58, 55,  1, 72, 62, 73],\n",
            "        [55, 65, 58,  1, 76, 54, 62, 67]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**nn.Linear(3,3,bias=False)** -: It is basically performing linear transformation. It's always to make sure that the nodes are lining up between input and output.\n",
        "3,3 indicates input layer has 3 nodes and ouput layer has 3 nodes\n",
        "\n",
        "weight * x + bias = y where x is input layer and y is output layer\n",
        "\n",
        "For additional info go to -> https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "**Softmax:** the softmax applies the standard exponential function to each element\n",
        "of the input vector (consisting of real numbers), and normalizes these values by dividing by the sum of all these exponentials.\n",
        "\n",
        "**nn.Embedding:** The length of the embedding is based on the vocab size and the embeddings are generated at the character level\n",
        "\n",
        "**Loss:** loss is calculated by taking the negative log likelihood. For instance, -ln(1/80)\n",
        "\n",
        "**Gradient descent:** gradient descent is used to change the weight matrix in a neural network basically to make the network better in such a way to reduce the loss and improve prediction accuracy.calculate the derivative(slope) from the current and move in a right direction based on that.\n",
        "   \n",
        "   Different Optimizers: Adamax,SGD,SparseAdam,Adamw\n",
        "  \n",
        "**Learning rate:** Lets, say now we know direction we want to move, LR basically tells us the step size, how fast the move in that direction is. Smaller learning rates train the neural network better compared to the bigger ones."
      ],
      "metadata": {
        "id": "XuLukEfQmD7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "7mReJZE81q-0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLargeModel(nn.Module):\n",
        "      def __init__(self,vocab_size):\n",
        "          super().__init__()\n",
        "          self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "          #logits are bascally the probability distribution of what we want to predict next\n",
        "      def forward(self,index,targets=None):\n",
        "          logits = self.token_embedding_table(index) # .shape, .view are basically to unpack and reshape the tensor objects\n",
        "        #The logits are basically the probability distribution of what we gonna predict next\n",
        "          if targets is None:\n",
        "                  loss = None\n",
        "          else:\n",
        "                  B, T, C = logits.shape  #c -> channels -. vocan size, #T - the next prediction, #B -> batch\n",
        "                  logits = logits.view(B*T, C)\n",
        "                  targets = targets.view(B*T)\n",
        "                  loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "          return logits, loss\n",
        "\n",
        "      def generate(self, index, max_new_tokens):\n",
        "            # index is (B, T) array of indices in the current context\n",
        "            for _ in range(max_new_tokens):\n",
        "                # get the predictions\n",
        "                logits, loss = self.forward(index)\n",
        "                # focus only on the last time step\n",
        "                logits = logits[:, -1, :] # becomes (B, C)\n",
        "                # apply softmax to get probabilities\n",
        "                probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "                # sample from the distribution\n",
        "                index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "                # append sampled index to the running sequence\n",
        "                index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "            return index\n",
        "\n",
        "model = BigramLargeModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "context = torch.zeros((2,2), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "id": "gRhN4hNil3yA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6d4e7c-1690-4001-e13c-b868b89c58e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "atR7_3WLnf[\n",
            "O;WM5hgQG!LlP6wUuui1ai wWv(0:[J0YupN\n",
            "KgMv?48\n",
            "UtJjoJ?B VM5pS\n",
            "bj]dqBw)3&bIR\n",
            "cN6wM5Pia\n",
            "&iiaO*DTQ\n",
            "H*RXnRdFN'Dm?﻿?DfsyHN82682Hm.?Q3:J72K3dbj(Y;A.6rnO;82[iqaWaWTs6oxpWIZs w4F.ZPOGd7nn?WA 2zh5]*WEt1t9j]xH﻿i_M1kGNY3&H6a59em,]﻿K'\n",
            "9(NmNqYK﻿s V*uap\n",
            "21e,rJ3mjS\"PJt9_MoP_u6Ak!RMEh*wx2gm*RWZ:'tF2FKLc6RY2pFcd4Ys\"RkeF7XoS?bp1a7keiKYskj]9ra0AP6NGdb]4(lW5QUQLY_,]_POYS5;-,?WKG_CH82'﻿59U*H9&. &jZCtyhp\n",
            "0u5ow*D zVH5Cr2Av.0yv29:]\"pgX4Vf&KISX,hCSr-srsPiD\"P6\"rQjKX!57TRZK7,kM3C18S9﻿,zbYs--7BfT\n",
            "bMxhw21knwZXPv!3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "fHfesJTSOOuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2ca62a-22d2-4702-9e0f-809e89ee36d7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 4.907, val loss: 4.923\n",
            "step: 250, train loss: 4.874, val loss: 4.850\n",
            "step: 500, train loss: 4.793, val loss: 4.780\n",
            "step: 750, train loss: 4.736, val loss: 4.731\n",
            "4.866482734680176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ovHcHHA1Rph",
        "outputId": "4dec7b9f-2fc2-4988-f2e0-1556c0a80a14"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1X]o﻿1fMxZUIBmiAvUH;]&KDO.EwpJ0UHW\"hg0!]y6vHz)1ke&sPN)?UUrLf&R.\n",
            "M,.ROF7VOnfRFLYZpSXa-2.O?jI1tD)X4;4FUqGcdb4j4YExB3!)Xu r8.Y&IA goPP0cMTKzcunA0(IWjS,Jz-2me&LYzX60)M-'rCDjrL(mEwkeYuQHsT\n",
            "bXoV(u2P'b4Wp0o1HO1Aq V;-]Lr?kwiqV[(coj]3C9OA m(M!m7-59xCrJBSpkehgl8&5CEt5cNq]72wBRZT K7,;UQj'z\"2jo﻿13sKSryWWDJ,(3XNy5Cn&A[ilBma?oPDZTf:\n",
            "0mLR.ZlvA8Z*B&i.!1-vo'qu5J)_UV _RtPmya3htDcQNHZAvSr(3C5tUQi&IXo0Wy,tD-0WzX8VG1AFKGYH)zuYwnEkD9\"dH9q.Ell\"*RKnEa\n",
            "i:\n",
            "cQ2W\"A9j.ZGO﻿EK,'Jo﻿-x5WUQ\n",
            "bVNmPUi2m(m  r'K_ 'e4YCXlXoQH;S2wBAQjS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NCsvDc8111Rz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}